{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Практическое задание №2 ","metadata":{}},{"cell_type":"markdown","source":"## Общая терминология по используемым данным\n\nПредоставляемые данные для разработки моделей и алгоритмов трекинга мяча в теннисе представляют собор набор игр (game), состоящих из нескольких клипов (clip), каждый из которых состоит из набора кадров (frame). Обратите внимание на структуру организации файлов внутри предоставляемого датасета для полного понимания.\n\nБольшинство алгоритмов трекинга объектов работают с несколькими последовательными кадрами, и в данном задании также подразумевается использование этого приема. Последовательность нескольких кадров будем именовать стопкой (stack), размер стопки (stack_s) является гиперпараметром разрабатываемого алгоритма.","metadata":{}},{"cell_type":"markdown","source":"# Заготовка решения","metadata":{}},{"cell_type":"markdown","source":"## Загрузка датасета\nДля работы с данными в ноутбуке kaggle необходимо подключить датасет. File -> Add or upload data, далее в поиске написать tennis-tracking-assignment и выбрать датасет. Если поиск не работает, то можно добавить датасет по url: https://www.kaggle.com/xubiker/tennistrackingassignment. После загрузки данные датасета будут примонтированы в ../input/tennistrackingassignment.","metadata":{}},{"cell_type":"markdown","source":"## Установка и импорт зависимостей","metadata":{}},{"cell_type":"markdown","source":"Установка необходимых пакетов (не забудьте \"включить интернет\" в настройках ноутбука kaggle):","metadata":{}},{"cell_type":"code","source":"!pip install moviepy --upgrade\n!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:04.192611Z","iopub.execute_input":"2022-12-22T20:49:04.193081Z","iopub.status.idle":"2022-12-22T20:49:36.568075Z","shell.execute_reply.started":"2022-12-22T20:49:04.192991Z","shell.execute_reply":"2022-12-22T20:49:36.566632Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting moviepy\n  Downloading moviepy-1.0.3.tar.gz (388 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting decorator<5.0,>=4.0.2\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.7/site-packages (from moviepy) (4.64.0)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.28.1)\nCollecting proglog<=1.0.0\n  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from moviepy) (1.21.6)\nRequirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.19.3)\nCollecting imageio_ffmpeg>=0.2.0\n  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio<3.0,>=2.5->moviepy) (9.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.12)\nBuilding wheels for collected packages: moviepy\n  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110743 sha256=4a0fee6619771f2781b2d53963e254a043e73319944c665de725b73a91d81c5c\n  Stored in directory: /root/.cache/pip/wheels/56/dc/2b/9cd600d483c04af3353d66623056fc03faed76b7518faae4df\nSuccessfully built moviepy\nInstalling collected packages: proglog, imageio_ffmpeg, decorator, moviepy\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Uninstalling decorator-5.1.1:\n      Successfully uninstalled decorator-5.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed decorator-4.4.2 imageio_ffmpeg-0.4.7 moviepy-1.0.3 proglog-0.1.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.7.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.12)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"<font color=red>\nПосле установки пакетов для корректной работы надо обязательно перезагрузить ядро. Run -> Restart and clear cell outputs. Без сего действа будет ошибка при попытке обращения к библиотеке moviepy при сохранении визуализации в виде видео. Может когда-то авторы библиотеки это починят...\n</font>","metadata":{}},{"cell_type":"markdown","source":"Импорт необходимых зависимостей:","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom typing import List, Tuple, Sequence\n\nimport numpy as np\nfrom numpy import unravel_index\nfrom PIL import Image, ImageDraw, ImageFont\nfrom tqdm import tqdm, notebook\n\nfrom moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n\nimport math\nfrom scipy.ndimage import gaussian_filter\n\nimport gc\nimport time\nimport random\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:36.571862Z","iopub.execute_input":"2022-12-22T20:49:36.572243Z","iopub.status.idle":"2022-12-22T20:49:37.309006Z","shell.execute_reply.started":"2022-12-22T20:49:36.572208Z","shell.execute_reply":"2022-12-22T20:49:37.307252Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Набор функций для загрузки данных из датасета","metadata":{}},{"cell_type":"markdown","source":"Функция load_clip_data загружает выбранный клип из выбранной игры и возвращает его в виде numpy массива [n_frames, height, width, 3] типа uint8. Для ускорения загрузки используется кэширование - однажды загруженные клипы хранятся на диске в виде npz архивов, при последующем обращении к таким клипам происходит загрузка npz архива.\n\n<font color=\"red\">\nТакже добавлена возможность чтения клипа в половинном разрешении 640x360, вместо оригинального 1280x720 для упрощения и ускорения разрабатываемых алгоритмов.\n</font>","metadata":{}},{"cell_type":"markdown","source":"Функция load_clip_labels загружает референсные координаты мяча в клипе в виде numpy массива [n_frames, 4], где в каждой строке массива содержатся значения [code, x, y, q]. x, y соответствуют координате центра мяча на кадре, q не используется в данном задании, code описывает статус мяча:\n* code = 0 - мяча в кадре нет\n* code = 1 - мяч присутствует в кадре и легко идентифицируем\n* code = 2 - мяч присутствует в кадре, но сложно идентифицируем\n* code = 3 - мяч присутствует в кадре, но заслонен другими объектами.\n\nПри загрузке в половинном разрешении координаты x, y делятся на 2.\n\nФункция load_clip загружает выбранный клип и соответствующий массив координат и возвращает их в виде пары.","metadata":{}},{"cell_type":"code","source":"def get_num_clips(path: Path, game: int) -> int:\n    return len(list((path / f'game{game}/').iterdir()))\n\n\ndef get_game_clip_pairs(path: Path, games: List[int]) -> List[Tuple[int, int]]:\n    return [(game, c)  for game in games for c in range(1, get_num_clips(path, game) + 1)]\n\n\ndef load_clip_data(path: Path, game: int, clip: int, downscale: bool, quiet=False, data_type=\"uint8\") -> np.ndarray:\n    if not quiet:\n        suffix = 'downscaled' if downscale else ''\n        print(f'loading clip data (game {game}, clip {clip}) {suffix}')\n    cache_path = path / 'cache'\n    cache_path.mkdir(exist_ok=True)\n    resize_code = '_ds2' if downscale else ''\n    cached_data_name = f'{game}_{clip}{resize_code}.npz'\n    if (cache_path / cached_data_name).exists():\n        clip_data = np.load(cache_path / cached_data_name)['clip_data']\n    else:\n        clip_path = path / f'game{game}/clip{clip}'\n        n_imgs = len(list(clip_path.iterdir())) - 1\n        imgs = [None] * n_imgs\n        for i in notebook.tqdm(range(n_imgs)):\n            img = Image.open(clip_path / f'{i:04d}.jpg')\n            if downscale:\n                img = img.resize((img.width // 2, img.height // 2),)\n            imgs[i] = np.array(img, dtype=data_type)\n        clip_data = np.stack(imgs)\n        cache_path.mkdir(exist_ok=True, parents=True)\n        np.savez_compressed(cache_path / cached_data_name, clip_data=clip_data)\n    return clip_data\n\n\ndef load_clip_labels(path: Path, game: int, clip: int, downscale: bool, quiet=False):\n    if not quiet:\n        print(f'loading clip labels (game {game}, clip {clip})')\n    clip_path = path / f'game{game}/clip{clip}'\n    labels = []\n    with open(clip_path / 'labels.csv') as csvfile:\n        lines = list(csv.reader(csvfile))\n        for line in lines[1:]:\n            values = np.array([-1 if i == '' else int(i) for i in line[1:]])\n            if downscale:\n                values[1] //= 2\n                values[2] //= 2\n            labels.append(values)\n    return np.stack(labels)\n\n\ndef load_clip(path: Path, game: int, clip: int, downscale: bool, quiet=False, data_type=\"uint8\"):\n    data = load_clip_data(path, game, clip, downscale, quiet, data_type)\n    labels = load_clip_labels(path, game, clip, downscale, quiet)\n    return data, labels\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.315871Z","iopub.execute_input":"2022-12-22T20:49:37.318793Z","iopub.status.idle":"2022-12-22T20:49:37.357729Z","shell.execute_reply.started":"2022-12-22T20:49:37.318747Z","shell.execute_reply":"2022-12-22T20:49:37.355112Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Набор дополнительных функций\n\nЕще несколько функций, немного облегчающих выполнение задания:\n\n* prepare_expariment создает новую директорию в out_path для хранения результатов текущего эксперимента. Нумерация выполняется автоматически, функция возвращает путь к созданной директории эксперимента;\n* ball_gauss_template - создает \"шаблон\" мяча, может быть использована в алгоритмах поиска мяча на изображении по корреляции;\n* create_masks - принимает набор кадров и набор координат мяча, и генерирует набор масок, в которых помещает шаблон мяча на заданные координаты. Может быть использована при обучении нейронной сети семантической сегментации;","metadata":{}},{"cell_type":"code","source":"def prepare_experiment(out_path: Path) -> Path:\n    out_path.mkdir(parents=True, exist_ok=True)\n    dirs = [d for d in out_path.iterdir() if d.is_dir() and d.name.startswith('exp_')]\n    experiment_id = max(int(d.name.split('_')[1]) for d in dirs) + 1 if dirs else 1\n    exp_path = out_path / f'exp_{experiment_id}'\n    exp_path.mkdir()\n    return exp_path\n\n\ndef ball_gauss_template(rad, sigma):\n    x, y = np.meshgrid(np.linspace(-rad, rad, 2 * rad + 1), np.linspace(-rad, rad, 2 * rad + 1)) \n    dst = np.sqrt(x * x + y * y) \n    gauss = np.exp(-(dst ** 2 / (2.0 * sigma ** 2)))     \n    return gauss\n\n\ndef create_masks(data: np.ndarray, labels: np.ndarray, resize):\n    rad = 64 #25\n    sigma = 10\n    if resize:\n        rad //= 2\n    ball = ball_gauss_template(rad, sigma)\n    n_frames = data.shape[0]\n    sh = rad\n    masks = []\n    for i in range(n_frames):\n        label = labels[i, ...] \n        frame = data[i, ...]\n        if 0 < label[0] < 3:\n            x, y = label[1:3]\n            mask = np.zeros((frame.shape[0] + 2 * rad + 2 * sh, frame.shape[1] + 2 * rad + 2 * sh), np.float32)\n            mask[y + sh : y + sh + 2 * rad + 1, x + sh : x + sh + 2 * rad + 1] = ball\n            mask = mask[rad + sh : -rad - sh, rad + sh : -rad - sh]\n            masks.append(mask)\n        else:\n            masks.append(np.zeros((frame.shape[0], frame.shape[1]), dtype=np.float32))\n    return np.stack(masks)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-12-22T20:49:37.365245Z","iopub.execute_input":"2022-12-22T20:49:37.365644Z","iopub.status.idle":"2022-12-22T20:49:37.378108Z","shell.execute_reply.started":"2022-12-22T20:49:37.365617Z","shell.execute_reply":"2022-12-22T20:49:37.377064Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Набор функций, предназначенных для визуализации результатов\n\nФункция visualize_prediction принимает набор кадров, набор координат детекции мяча (можно подавать как референсные значения, так и предсказанные) и создает видеоклип, в котором отрисовывается положение мяча, его трек, номер кадра и метрика качества трекинга (если она была передана в функцию). Видеоклип сохраняется в виде mp4 файла. Кроме того данная функция создает текстовый файл, в который записывает координаты детекции мяча и значения метрики качества трекинга.\n\nФункция visualize_prob принимает набор кадров и набор предсказанных карт вероятности и создает клип с наложением предсказанных карт вероятности на исходные карты. Области \"подсвечиваются\" желтым, клип сохраняется в виде mp4 видеофайла. Данная функция может быть полезна при наличии в алгоритме трекинга сети, осуществляющей семантическую сегментацию.","metadata":{}},{"cell_type":"code","source":"def _add_frame_number(frame: np.ndarray, number: int) -> np.ndarray:\n    fnt = ImageFont.load_default() # ImageFont.truetype(\"arial.ttf\", 25)\n    img = Image.fromarray(frame)\n    draw = ImageDraw.Draw(img)\n    draw.text((10, 10), f'frame {number}', font=fnt, fill=(255, 0, 255))\n    return np.array(img)\n\n\ndef _vis_clip(data: np.ndarray, lbls: np.ndarray, metrics: List[float] = None, ball_rad=5, color=(255, 0, 0), track_length=10):\n    print('perfoming clip visualization')\n    n_frames = data.shape[0]\n    frames_res = []\n    fnt = ImageFont.load_default() # ImageFont.truetype(\"arial.ttf\", 25)\n    for i in range(n_frames):\n        img = Image.fromarray(data[i, ...])\n        draw = ImageDraw.Draw(img)\n        txt = f'frame {i}'\n        if metrics is not None:\n            txt += f', SiBaTrAcc: {metrics[i]:.3f}'\n        draw.text((10, 10), txt, font=fnt, fill=(255, 0, 255))\n        label = lbls[i]\n        if label[0] != 0: # the ball is clearly visible\n            px, py = label[1], label[2]\n            draw.ellipse((px - ball_rad, py - ball_rad, px + ball_rad, py + ball_rad), outline=color, width=2)\n            for q in range(track_length):\n                if lbls[i-q-1][0] == 0:\n                    break\n                if i - q > 0:\n                    draw.line((lbls[i - q - 1][1], lbls[i - q - 1][2], lbls[i - q][1], lbls[i - q][2]), fill=color)                \n        frames_res.append(np.array(img))\n    return frames_res\n\n\ndef _save_clip(frames: Sequence[np.ndarray], path: Path, fps):\n    assert path.suffix in ('.mp4', '.gif')\n    clip = ImageSequenceClip(frames, fps=fps)\n    if path.suffix == '.mp4':\n        clip.write_videofile(str(path), fps=fps, logger=None)\n    else:\n        clip.write_gif(str(path), fps=fps, logger=None)\n\n\ndef _to_yellow_heatmap(frame: np.ndarray, pred_frame: np.ndarray, alpha=0.4):\n    img = Image.fromarray((frame * alpha).astype(np.uint8))\n    maskR = (pred_frame * (1 - alpha) * 255).astype(np.uint8)\n    maskG = (pred_frame * (1 - alpha) * 255).astype(np.uint8)\n    maskB = np.zeros_like(maskG, dtype=np.uint8)\n    mask = np.stack([maskR, maskG, maskB], axis=-1)\n    return img + mask\n\n\ndef _vis_pred_heatmap(data_full: np.ndarray, pred_prob: np.ndarray, display_frame_number):\n    n_frames = data_full.shape[0]\n    v_frames = []\n    for i in range(n_frames):\n        frame = data_full[i, ...]\n        pred = pred_prob[i, ...]\n        hm = _to_yellow_heatmap(frame, pred)\n        if display_frame_number:\n            hm = _add_frame_number(hm, i)\n        v_frames.append(hm)\n    return v_frames\n\n\ndef visualize_prediction(data_full: np.ndarray, labels_pr: np.ndarray, save_path: Path, name: str, metrics=None, fps=15):\n    with open(save_path / f'{name}.txt', mode='w') as f:\n        if metrics is not None:\n            f.write(f'SiBaTrAcc: {metrics[-1]} \\n')\n        for i in range(labels_pr.shape[0]):\n            f.write(f'frame {i}: {labels_pr[i, 0]}, {labels_pr[i, 1]}, {labels_pr[i, 2]} \\n')\n\n    v = _vis_clip(data_full, labels_pr, metrics)\n    _save_clip(v, save_path / f'{name}.mp4', fps=fps)\n\n\ndef visualize_prob(data: np.ndarray, pred_prob: np.ndarray, save_path: Path, name: str, frame_number=True, fps=15):\n    v_pred = _vis_pred_heatmap(data, pred_prob, frame_number)\n    _save_clip(v_pred, save_path / f'{name}_prob.mp4', fps=fps)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.379685Z","iopub.execute_input":"2022-12-22T20:49:37.380070Z","iopub.status.idle":"2022-12-22T20:49:37.401672Z","shell.execute_reply.started":"2022-12-22T20:49:37.380035Z","shell.execute_reply":"2022-12-22T20:49:37.400794Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# clip = load_clip_data(Path('../input/tennistrackingassignment/train/'), 1, 1, downscale=True)\n# labels = load_clip_labels(Path('../input/tennistrackingassignment/train/'), 1, 1, downscale=True)\n# visualize_prediction(clip, labels, Path('/kaggle/working'), \"Test\")","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.403185Z","iopub.execute_input":"2022-12-22T20:49:37.403631Z","iopub.status.idle":"2022-12-22T20:49:37.414438Z","shell.execute_reply.started":"2022-12-22T20:49:37.403596Z","shell.execute_reply":"2022-12-22T20:49:37.413164Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Класс DataGenerator \n\nКласс, отвечающий за генерацию данных для обучения модели. Принимает на вход путь к директории с играми, индексы игр, используемые для генерации данных, и размер стопки. Хранит в себе автоматически обновляемый пул с клипами игр.\n\nВ пуле содержится pool_s клипов. DataGenerator позволяет генерировать батч из стопок (размера stack_s) последовательных кадров. Выбор клипа для извлечения данных взвешенно-случайный: чем больше длина клипа по сравнению с другими клипами в пуле, тем вероятнее, что именно из него будет сгенерирована стопка кадров. Выбор стопки кадров внтури выбранного клипа полностью случаен. Кадры внутри стопки конкатенируются по последнему измерению (каналам).\n\nПосле генерирования количества кадров равного общему количеству кадров, хранимых в пуле, происходит автоматическое обновление пула: из пула извлекаются pool_update_s случайных клипов, после чего в пул загружается pool_update_s случайных клипов, не присутствующих в пуле. В случае, если размер пула pool_s больше или равен суммарному количеству клипов в играх, переданных в конструктор, все клипы сразу загружаются в пул, и автообновление не производится.\n\nИспользование подобного пула позволяет работать с практически произвольным количеством клипов, без необходимости загружать их всех в оперативную память.","metadata":{}},{"cell_type":"markdown","source":"Для вашего удобства функция извлечения стопки кадров из пула помимо самой стопки также создает и возвращает набор сгенерированных масок с мячом исходя из референсных координат мяча в клипе.\n\nФункция random_g принимает гиперпараметр размера стопки кадров и предоставляет генератор, возвращающий стопки кадров и соответствующие им маски. Данный генератор может быть использован при реализации решения на tensorflow. Обновление пула происходит автоматически, об этом беспокоиться не нужно.","metadata":{}},{"cell_type":"code","source":"class DataGenerator:\n\n    def __init__(self, path: Path, games: List[int], stack_s, downscale,\n                 pool_s=30, pool_update_s=10, pool_autoupdate=True,\n                 quiet=False, add_padding=False, binarize_mask=False,\n                 move_axis=False, flatten=False) -> None:\n        self.path = path\n        self.stack_s = stack_s\n        self.downscale = downscale\n        self.pool_size = pool_s\n        self.pool_update_size = pool_update_s\n        self.pool_autoupdate = pool_autoupdate\n        self.quiet = quiet\n        self.add_padding = add_padding\n        self.binarize_mask = binarize_mask\n        self.move_axis = move_axis\n        self.flatten = flatten\n        \n        self.data = []\n        self.masks = []\n\n        self.frames_in_pool = 0\n        self.produced_frames = 0\n        self.game_clip_pairs = get_game_clip_pairs(path, list(set(games)))\n        self.game_clip_pairs_loaded = []\n        self.game_clip_pairs_not_loaded = list.copy(self.game_clip_pairs) \n        self.pool = {}\n\n        self._first_load()\n\n    def _first_load(self):\n        # --- if all clips can be placed into pool at once, there is no need to refresh pool at all ---\n        if len(self.game_clip_pairs) <= self.pool_size:\n            for gcp in self.game_clip_pairs:\n                self._load(gcp)\n            self.game_clip_pairs_loaded = list.copy(self.game_clip_pairs)\n            self.game_clip_pairs_not_loaded.clear()\n            self.pool_autoupdate = False\n        else:\n            self._load_to_pool(self.pool_size)        \n        self._update_clip_weights()\n\n    def _load(self, game_clip_pair):\n        game, clip = game_clip_pair\n        data, labels = load_clip(self.path, game, clip, self.downscale, quiet=self.quiet)\n        masks = create_masks(data, labels, self.downscale)\n        masks = masks.reshape(masks.shape + (1, ))\n        if self.add_padding:\n            pad_size = ((data.shape[1] // 32 + 1) * 32 - data.shape[1]) // 2\n            data = np.pad(data, [(0, 0), (pad_size, pad_size), (0, 0), (0, 0)])\n            masks = np.pad(masks, [(0, 0), (pad_size, pad_size), (0, 0)])\n        if self.binarize_mask:\n            masks[masks != 0] = 1\n            masks = masks.astype(\"uint8\")\n        if self.move_axis:\n            data = np.moveaxis(data, -1, 1)\n            masks = np.expand_dims(masks, axis=1)\n        if self.flatten:\n            masks = masks.reshape((masks.shape[0], -1, 1))\n        weight = data.shape[0] if data.shape[0] >= self.stack_s else 0\n        self.pool[game_clip_pair] = (data, labels, masks, weight)\n        self.frames_in_pool += data.shape[0] - self.stack_s + 1\n        # print(f'items in pool: {len(self.pool)} - {self.pool.keys()}')\n\n    def _remove(self, game_clip_pair):\n        value = self.pool.pop(game_clip_pair)\n        self.frames_in_pool -= value[0].shape[0] - self.stack_s + 1\n        del value\n        # print(f'items in pool: {len(self.pool)} - {self.pool.keys()}')\n\n    def _update_clip_weights(self):\n        weights = [self.pool[pair][-1] for pair in self.game_clip_pairs_loaded]\n        tw = sum(weights)\n        self.clip_weights = [w / tw for w in weights]\n        # print(f'clip weights: {self.clip_weights}')\n\n    def _remove_from_pool(self, n):\n        # --- remove n random clips from pool ---\n        if len(self.game_clip_pairs_loaded) >= n:\n            remove_pairs = random.sample(self.game_clip_pairs_loaded, n)\n            for pair in remove_pairs:\n                self._remove(pair)\n                self.game_clip_pairs_loaded.remove(pair)\n                self.game_clip_pairs_not_loaded.append(pair)\n            gc.collect()\n\n    def _load_to_pool(self, n):\n        # --- add n random clips to pool ---\n        gc.collect()\n        add_pairs = random.sample(self.game_clip_pairs_not_loaded, n)\n        for pair in add_pairs:\n            self._load(pair)\n            self.game_clip_pairs_not_loaded.remove(pair)\n            self.game_clip_pairs_loaded.append(pair)\n\n    def update_pool(self):\n        self._remove_from_pool(self.pool_update_size)\n        self._load_to_pool(self.pool_update_size)\n        self._update_clip_weights()\n\n    def get_random_stack(self):\n        pair_idx = np.random.choice(len(self.game_clip_pairs_loaded), 1, p=self.clip_weights)[0]\n        game_clip_pair = self.game_clip_pairs_loaded[pair_idx]\n        d, _, m, _ = self.pool[game_clip_pair]\n        start = np.random.choice(d.shape[0] - self.stack_s, 1)[0]\n        frames_stack = d[start : start + self.stack_s, ...]\n        frames_stack = np.squeeze(np.split(frames_stack, indices_or_sections=self.stack_s, axis=0))\n        if self.move_axis:\n            frames_stack = np.concatenate(frames_stack, axis=0)\n        else:\n            frames_stack = np.concatenate(frames_stack, axis=-1)\n        mask = m[start + self.stack_s - 1, ...]\n        return frames_stack, mask\n\n    def get_random_batch(self, batch_s):\n        imgs, masks = [], []\n        while len(imgs) < batch_s:\n            frames_stack, mask = self.get_random_stack()\n            imgs.append(frames_stack)\n            masks.append(mask)\n        if self.pool_autoupdate:\n            self.produced_frames += batch_s\n            # print(f'produced frames: {self.produced_frames} from {self.frames_in_pool}')\n            if self.produced_frames >= self.frames_in_pool:\n                self.update_pool()\n                self.produced_frames = 0\n        return np.stack(imgs), np.stack(masks)\n\n    def random_g(self, batch_s):\n        while True:\n            imgs_batch, masks_batch = self.get_random_batch(batch_s)\n            yield imgs_batch, masks_batch","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.416032Z","iopub.execute_input":"2022-12-22T20:49:37.416608Z","iopub.status.idle":"2022-12-22T20:49:37.443014Z","shell.execute_reply.started":"2022-12-22T20:49:37.416574Z","shell.execute_reply":"2022-12-22T20:49:37.442125Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Пример использования DataGenerator","metadata":{}},{"cell_type":"markdown","source":"Рекомендованный размер пула pool_s=10 в случае использования уменьшенных вдвое изображений. При большем размере пула есть большая вероятность нехватки имеющихся 13G оперативной памяти.\nИспользуйте параметр quiet=True в конструкторе DataGenerator, если хотите скрыть все сообщения о чтении данных и обновлении пула.","metadata":{}},{"cell_type":"code","source":"# stack_s = 3\n# batch_s = 4\n# train_gen = DataGenerator(Path('../input/tennistrackingassignment/train/'), [1, 2, 3, 4], stack_s=stack_s, downscale=True, pool_s=10, pool_update_s=4, quiet=False, move_axis=True)\n\n# for i in range(10):\n#     imgs, masks = train_gen.get_random_batch(batch_s)\n#     print(imgs.shape, imgs.dtype, masks.shape, masks.dtype)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.444605Z","iopub.execute_input":"2022-12-22T20:49:37.445430Z","iopub.status.idle":"2022-12-22T20:49:37.455613Z","shell.execute_reply.started":"2022-12-22T20:49:37.445396Z","shell.execute_reply":"2022-12-22T20:49:37.454335Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# stack_s = 3\n# train_gen = DataGenerator(Path('../input/tennistrackingassignment/train/'), [1], stack_s=stack_s, downscale=True, pool_s=10, pool_update_s=4, quiet=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.457123Z","iopub.execute_input":"2022-12-22T20:49:37.457692Z","iopub.status.idle":"2022-12-22T20:49:37.465319Z","shell.execute_reply.started":"2022-12-22T20:49:37.457659Z","shell.execute_reply":"2022-12-22T20:49:37.464240Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# stack, mask = train_gen.get_random_stack(add_pad=True)\n# print(stack.shape, mask.shape)\n\n# for i in range(stack_s):\n#     plt.figure()\n#     plt.imshow(stack[:, :, 3 * i: 3 * i + 3])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.469502Z","iopub.execute_input":"2022-12-22T20:49:37.469883Z","iopub.status.idle":"2022-12-22T20:49:37.478117Z","shell.execute_reply.started":"2022-12-22T20:49:37.469835Z","shell.execute_reply":"2022-12-22T20:49:37.476999Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Класс Metrics\nКласс для вычисления метрики качества трекинга SiBaTrAcc. Функция evaluate_predictions принимает массив из референсных и предсказанных координат мяча для клипа и возвращает массив аккумулированных значений SiBaTrAcc (может быть полезно для визуализации результатов предсказания) и итоговое значение метрики SiBaTrAcc.","metadata":{}},{"cell_type":"code","source":"class Metrics:\n\n    @staticmethod\n    def position_error(label_gt: np.ndarray, label_pr: np.ndarray, step=8, alpha=1.5, e1=5, e2=5):\n        # gt codes:\n        # 0 - the ball is not within the image\n        # 1 - the ball can easily be identified\n        # 2 - the ball is in the frame, but is not easy to identify\n        # 3 - the ball is occluded\n        if label_gt[0] != 0 and label_pr[0] == 0:\n            return e1\n        if label_gt[0] == 0 and label_pr[0] != 0:\n            return e2\n        dist = math.sqrt((label_gt[1] - label_pr[1]) ** 2 + (label_gt[2] - label_pr[2]) ** 2)\n        pe = math.floor(dist / step) ** alpha\n        pe = min(pe, 5)\n        return pe\n\n    @staticmethod\n    def evaluate_predictions(labels_gt, labels_pr) -> Tuple[List[float], float]:\n        pe = [Metrics.position_error(labels_gt[i, ...], labels_pr[i, ...]) for i in range(len(labels_gt))]\n        SIBATRACC = []\n        for i, _ in enumerate(pe):\n            SIBATRACC.append(1 - sum(pe[: i + 1]) / ((i + 1) * 5))\n        SIBATRACC_total = 1 - sum(pe) / (len(labels_gt) * 5)\n        return SIBATRACC, SIBATRACC_total\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.479642Z","iopub.execute_input":"2022-12-22T20:49:37.480119Z","iopub.status.idle":"2022-12-22T20:49:37.490714Z","shell.execute_reply.started":"2022-12-22T20:49:37.480085Z","shell.execute_reply":"2022-12-22T20:49:37.489887Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Основной класс модели SuperTrackingModel\n\nРеализует всю логику обучения, сохранения, загрузки и тестирования разработанной модели трекинга. Этот класс можно и нужно расширять.\n\nВ качестве примера вам предлагается заготовка модели, в которой трекинг осуществляется за счет предсказания маски по входному батчу и последующему предсказанию координат мяча по полученной маски. В данном варианте вызов функции предсказания координат по клипу (predict) повлечет за собой разбиение клипа на батчи, вызов предсказания маски для каждого батча, склеивание результатов в последовательность масок, вызов функции по вычислению координат мяча по маскам и возвращения результата. Описанные действия уже реализованы, вам остается только написать функции predict_on_bath и get_labels_from_prediction. Эта же функция predict используется и в вызове функции test, дополнительно вычисляя метрику качества трекинга и при необходимости визуализируя результат тестирования. Обратите внимание, что в результирующем numpy массиве с координатами помимо значений x и y первым значением в каждой строке должно идти значение code (0, если мяча в кадре нет и > 0, если мяч в кадре есть) для корректного вычисления качества трекинга.\n\n<font color=\"red\">\nВам разрешается менять логику работы класса модели, (например, если решение не подразумевает использование масок), но при этом логика и работа функций load и test должна остаться неизменной!\n</font>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Activation, MaxPool2D, Concatenate, Reshape, Permute\nfrom skimage.morphology import binary_erosion","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:37.492077Z","iopub.execute_input":"2022-12-22T20:49:37.492937Z","iopub.status.idle":"2022-12-22T20:49:42.420768Z","shell.execute_reply.started":"2022-12-22T20:49:37.492902Z","shell.execute_reply":"2022-12-22T20:49:42.419813Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:42.422348Z","iopub.execute_input":"2022-12-22T20:49:42.423024Z","iopub.status.idle":"2022-12-22T20:49:42.545164Z","shell.execute_reply.started":"2022-12-22T20:49:42.422988Z","shell.execute_reply":"2022-12-22T20:49:42.544252Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:42.546649Z","iopub.execute_input":"2022-12-22T20:49:42.547021Z","iopub.status.idle":"2022-12-22T20:49:42.552197Z","shell.execute_reply.started":"2022-12-22T20:49:42.546986Z","shell.execute_reply":"2022-12-22T20:49:42.550937Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def TrackNet( n_classes=1,  input_height=360, input_width=640): # input_height = 360, input_width = 640\n\n    imgs_input = Input(shape=(input_height,input_width, 9))\n\n    #layer1\n    x = ( BatchNormalization())(imgs_input)\n    x = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer2\n    x = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer3\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n    #layer4\n    x = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer5\n    x = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer6\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n    #layer7\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer8\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer9\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer10\n    x = MaxPooling2D((2, 2), strides=(2, 2),   )(x)\n\n    #layer11\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer12\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer13\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer14\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer15\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer16\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer17\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer18\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer19\n    x = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer20\n    x = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer21\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer22\n    x = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'  ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer23\n    x = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer24\n    x = ( Conv2D( n_classes , (3, 3) , kernel_initializer='random_uniform', padding='same'))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    o_shape = Model(imgs_input, x).output_shape\n    print(\"layer24 output shape:\", o_shape[1],o_shape[2],o_shape[3])\n    #layer24 output shape: 256, 360, 640\n\n    OutputHeight = o_shape[1]\n    OutputWidth = o_shape[2]\n    \n    #reshape the size to (256, 360*640)\n    x = (Reshape((OutputHeight*OutputWidth,  -1)))(x)\n\n#     #change dimension order to (360*640, 256)\n#     x = (Permute((2, 1)))(x)\n    \n    #layer25\n    gaussian_output = (Activation('sigmoid'))(x)\n    \n    model = Model( imgs_input , gaussian_output )\n    model.outputWidth = OutputWidth\n    model.outputHeight = OutputHeight\n    \n    #show model's details\n    #model.summary()\n\n    return model\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:42.554012Z","iopub.execute_input":"2022-12-22T20:49:42.554761Z","iopub.status.idle":"2022-12-22T20:49:42.578525Z","shell.execute_reply.started":"2022-12-22T20:49:42.554726Z","shell.execute_reply":"2022-12-22T20:49:42.577572Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"TrackNet()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:42.580445Z","iopub.execute_input":"2022-12-22T20:49:42.581152Z","iopub.status.idle":"2022-12-22T20:49:46.079284Z","shell.execute_reply.started":"2022-12-22T20:49:42.581118Z","shell.execute_reply":"2022-12-22T20:49:46.078236Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2022-12-22 20:49:42.682697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:42.772034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:42.772832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:42.774098: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-22 20:49:42.774439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:42.775145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:42.775780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:45.123065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:45.124231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:45.125238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-22 20:49:45.126141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"layer24 output shape: 360 640 1\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.engine.functional.Functional at 0x7f822d3c2050>"},"metadata":{}}]},{"cell_type":"code","source":"def weighted_cross_entropy(beta):\n    def loss(y_true, y_pred):\n        weight_a = beta * tf.cast(y_true, tf.float32)\n        weight_b = 1 - tf.cast(y_true, tf.float32)\n        o = (tf.math.log1p(tf.exp(-tf.abs(y_pred))) + tf.nn.relu(-y_pred)) * (weight_a + weight_b) + y_pred * weight_b \n        return tf.reduce_mean(o)\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:46.080587Z","iopub.execute_input":"2022-12-22T20:49:46.081265Z","iopub.status.idle":"2022-12-22T20:49:46.089018Z","shell.execute_reply.started":"2022-12-22T20:49:46.081227Z","shell.execute_reply":"2022-12-22T20:49:46.087754Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class SuperTrackingModel:\n\n    def __init__(self, batch_s, stack_s, out_path, downscale):\n        self.batch_s = batch_s\n        self.stack_s = stack_s\n        self.out_path = out_path\n        self.downscale = downscale\n        self.model = TrackNet()\n        \n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n                        initial_learning_rate=10.0,\n                        decay_steps=1000,\n                        decay_rate=0.9)\n        \n        loss = tf.losses.BinaryCrossentropy()\n\n        self.model.compile(loss=loss, optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0), metrics=['accuracy'])\n\n\n\n    def load(self):\n        # todo: add code for loading model here\n        print('Running stub for loading model ...')\n        url = f'https://drive.google.com/drive/folders/1B4UkhSOSQVzRRXjA8Hukap7boCq6LyqE?usp=sharing'\n        gdown.download_folder(url, quiet=False, use_cookies=False)      \n        self.model = tf.keras.models.load_model(Path('/fast'))\n        print('Loading model done.')\n\n    def predict_on_batch(self, batch: np.ndarray) -> np.ndarray:\n        \n        # print(f\"Predicting on batch with shape {batch.shape}\")\n        result = self.model.predict(batch)\n        # print(f\"Got prediction with shape: {result.shape}\")\n        return result.reshape(self.batch_s, 360, 640)\n    \n    \n    def _predict_prob_on_clip(self, clip: np.ndarray) -> np.ndarray:\n        print('doing predictions')\n        n_frames = clip.shape[0]\n        # --- get stacks ---\n        stacks = []\n        for i in range(n_frames - self.stack_s + 1):\n            stack = clip[i : i + self.stack_s, ...]\n            stack = np.squeeze(np.split(stack, self.stack_s, axis=0))\n            stack = np.concatenate(stack, axis=-1)\n            stacks.append(stack)\n        # --- round to batch size ---\n        add_stacks = 0\n        while len(stacks) % self.batch_s != 0:\n            stacks.append(stacks[-1])\n            add_stacks += 1\n        # --- group into batches ---\n        batches = []\n        for i in range(len(stacks) // self.batch_s):\n            batch = np.stack(stacks[i * self.batch_s : (i + 1) * self.batch_s])\n            batches.append(batch)\n        stacks.clear()\n        # --- perform predictions ---\n        predictions = []\n        for batch in batches:\n            pred = np.squeeze(self.predict_on_batch(batch))\n            predictions.append(pred)\n        # --- crop back to source length ---\n        predictions = np.concatenate(predictions, axis=0)\n        if (add_stacks > 0):\n            predictions = predictions[:-add_stacks, ...]\n        batches.clear()\n        # --- add (stack_s - 1) null frames at the begining ---\n        start_frames = np.zeros((stack_s - 1, predictions.shape[1], predictions.shape[2]), dtype=np.float32)\n        predictions = np.concatenate((start_frames, predictions), axis=0)\n        print('predictions are made')        \n        return predictions\n\n    def get_labels_from_prediction(self, pred_prob: np.ndarray, upscale_coords: bool) -> np.ndarray:\n        # todo: get ball coordinates from predicted masks\n        # remember to upscale predicted coords if you use downscaled images\n        # print(f\"Acquiring coordinates from a segmentation mask with shape: {pred_prob.shape}\")\n            \n        n_frames = pred_prob.shape[0]\n        coords = np.zeros([n_frames, 3])\n        for i in range(n_frames):\n            prob_on_frame = pred_prob[i]\n            prob_on_frame[prob_on_frame > 0.1] = 255.0\n            prob_on_frame = prob_on_frame.astype(\"uint8\")\n            prob_on_frame = binary_erosion(prob_on_frame, np.ones(shape=(50, 50)))\n            if not np.any(prob_on_frame):\n                coords[i] = [0, 0, 0]\n            else:\n                x, y = np.median(np.argwhere(prob_on_frame), axis=0)\n                if upscale_coords:\n                    x *= 2\n                    y *= 2\n                coords[i] = [1, x, y]\n        return coords\n\n    def predict(self, clip: np.ndarray, upscale_coords=True) -> np.ndarray:\n        prob_pr = self._predict_prob_on_clip(clip)\n        labels_pr = self.get_labels_from_prediction(prob_pr, upscale_coords)\n        return labels_pr, prob_pr\n\n    def test(self, data_path: Path, games: List[int], do_visualization=False, test_name='test'):\n        game_clip_pairs = get_game_clip_pairs(data_path, games)\n        SIBATRACC_vals = []\n        for game, clip in game_clip_pairs:\n            data = load_clip_data(data_path, game, clip, downscale=self.downscale)\n            if do_visualization:\n                data_full = load_clip_data(data_path, game, clip, downscale=False) if self.downscale else data\n            labels_gt = load_clip_labels(data_path, game, clip, downscale=False)\n            labels_pr, prob_pr = self.predict(data)\n            SIBATRACC_per_frame, SIBATRACC_total = Metrics.evaluate_predictions(labels_gt, labels_pr)\n            SIBATRACC_vals.append(SIBATRACC_total)\n            if do_visualization:\n                visualize_prediction(data_full, labels_pr, self.out_path, f'{test_name}_g{game}_c{clip}', SIBATRACC_per_frame)\n                visualize_prob(data, prob_pr, self.out_path, f'{test_name}_g{game}_c{clip}')\n                del data_full\n            del data, labels_gt, labels_pr, prob_pr\n            gc.collect()\n        SIBATRACC_final = sum(SIBATRACC_vals) / len(SIBATRACC_vals)\n        return SIBATRACC_final\n\n    def train(self, train_gen, val_gen, steps_num, validation_steps_num, epoch_num=5, param_6=None):\n        \n\n        # todo: implement model training here\n        print('Running stub for training model...')\n        self.model.fit(train_gen(self.batch_s),\n                       validation_data=val_gen(self.batch_s),\n                       validation_steps=validation_steps_num,\n                       steps_per_epoch=steps_num,\n                       epochs=epoch_num)\n        \n        print('training done.')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:49:46.090806Z","iopub.execute_input":"2022-12-22T20:49:46.091177Z","iopub.status.idle":"2022-12-22T20:49:46.115488Z","shell.execute_reply.started":"2022-12-22T20:49:46.091142Z","shell.execute_reply":"2022-12-22T20:49:46.114415Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"\nПример пайплайна для обучения модели:","metadata":{}},{"cell_type":"code","source":"batch_s = 4\nstack_s = 3\ndownscale = True","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:50:27.034824Z","iopub.execute_input":"2022-12-22T20:50:27.035212Z","iopub.status.idle":"2022-12-22T20:50:27.042058Z","shell.execute_reply.started":"2022-12-22T20:50:27.035180Z","shell.execute_reply":"2022-12-22T20:50:27.039763Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"output_path = prepare_experiment(Path('/kaggle/working'))","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:50:27.341406Z","iopub.execute_input":"2022-12-22T20:50:27.342067Z","iopub.status.idle":"2022-12-22T20:50:27.348869Z","shell.execute_reply.started":"2022-12-22T20:50:27.342025Z","shell.execute_reply":"2022-12-22T20:50:27.347891Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n\ntrain_gen = DataGenerator(Path('../input/tennistrackingassignment/train/'),\n                          [1, 2, 3], stack_s=stack_s, downscale=True,\n                          pool_s=5, pool_update_s=4, quiet=True, flatten=True, binarize_mask=True)\nval_gen = DataGenerator(Path('../input/tennistrackingassignment/train/'),\n                        [4], stack_s=stack_s, downscale=True,\n                        pool_s=2, pool_update_s=2, quiet=True, flatten=True, binarize_mask=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SuperTrackingModel(4, stack_s, out_path=output_path, downscale=downscale)\nmodel.train(train_gen.random_g, val_gen.random_g, 50, 2, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train(train_gen.random_g, val_gen.random_g, 50, 10, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iterator = train_gen.random_g(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, mask = next(iterator)\nprint(mask.shape, image.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image, mask = next(iterator)\nnp.unique(image)\nplt.imshow(image[0][:,:, 6:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, mask = next(iterator)\nplt.imshow(image[0][:,:, :3])\nplt.show()\nres = model.model.predict_on_batch(image)\nplt.imshow(res[0].reshape((360, 640, 1)), cmap=\"gray\")\nplt.show()\nplt.imshow(mask[0].reshape((360, 640, 1)), cmap=\"gray\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.model.save(Path('/kaggle/working/fast'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SuperTrackingModel(batch_s, stack_s, out_path=output_path, downscale=downscale)\nmodel.load()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob_pr = model._predict_prob_on_clip(data)\nprint(prob_pr.shape)\nvisualize_prob(data, prob_pr, Path('/kaggle/working'), f'help_g1_c1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Пример пайплайна для тестирования обученной модели:","metadata":{}},{"cell_type":"code","source":"new_model = SuperTrackingModel(4, stack_s, out_path=output_path, downscale=downscale)\nnew_model.load()","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:50:36.515795Z","iopub.execute_input":"2022-12-22T20:50:36.516177Z","iopub.status.idle":"2022-12-22T20:50:40.570040Z","shell.execute_reply.started":"2022-12-22T20:50:36.516147Z","shell.execute_reply":"2022-12-22T20:50:40.568984Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"layer24 output shape: 360 640 1\nRunning stub for loading model ...\nLoading model done.\n","output_type":"stream"}]},{"cell_type":"code","source":"sibatracc_final = new_model.test(Path('../input/tennistrackingassignment/test/'), [1,], do_visualization=True, test_name='test')\nprint(f'SiBaTrAcc final value: {sibatracc_final}')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:50:40.572126Z","iopub.execute_input":"2022-12-22T20:50:40.572541Z","iopub.status.idle":"2022-12-22T20:51:46.264349Z","shell.execute_reply.started":"2022-12-22T20:50:40.572501Z","shell.execute_reply":"2022-12-22T20:51:46.262806Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"loading clip data (game 1, clip 1) downscaled\nloading clip data (game 1, clip 1) \nloading clip labels (game 1, clip 1)\ndoing predictions\n","output_type":"stream"},{"name":"stderr","text":"2022-12-22 20:50:54.835662: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-12-22 20:50:56.011109: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"predictions are made\nperfoming clip visualization\nMoviepy - Building video /kaggle/working/exp_4/test_g1_c1.mp4.\nMoviepy - Writing video /kaggle/working/exp_4/test_g1_c1.mp4\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1513751841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msibatracc_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/tennistrackingassignment/test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_visualization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'SiBaTrAcc final value: {sibatracc_final}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3167533914.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, data_path, games, do_visualization, test_name)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mSIBATRACC_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSIBATRACC_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_visualization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mvisualize_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{test_name}_g{game}_c{clip}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIBATRACC_per_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mvisualize_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{test_name}_g{game}_c{clip}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mdata_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1585180741.py\u001b[0m in \u001b[0;36mvisualize_prediction\u001b[0;34m(data_full, labels_pr, save_path, name, metrics, fps)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vis_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0m_save_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf'{name}.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1585180741.py\u001b[0m in \u001b[0;36m_save_clip\u001b[0;34m(frames, path, fps)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSequenceClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.mp4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mevaldict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mevaldict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    133\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mevaldict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    305\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                            logger=logger)\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                 \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbitrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0maudiofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudiofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                                 ffmpeg_params=ffmpeg_params) as writer:\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mnframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m'-s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%dx%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;34m'-pix_fmt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rgba'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb24'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%.02f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;34m'-an'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         ]\n","\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"],"ename":"TypeError","evalue":"must be real number, not NoneType","output_type":"error"}]},{"cell_type":"markdown","source":"Во время самостоятельного тестирования попробуйте хотя бы раз сделать тестирование с визуализацией (do_visualization=True), чтобы визуально оценить качество трекинга разработанной моделью.","metadata":{}},{"cell_type":"markdown","source":"<font color=red>\nЗагрузка модели через функцию load должна происходить полностью автоматически без каких-либо действий со стороны пользователя! Один из вариантов подобной реализации с использованием google drive и пакета gdown приведен в разделе с дополнениями.\n</font>","metadata":{}},{"cell_type":"markdown","source":"## Дополнения\n\nИногда при записи большого количества файлов в output директорию kaggle может \"тупить\" и не отображать корректно структуру дерева файлов в output и не показывать кнопки для скачивания выбранного файла. В этом случае удобно будет запаковать директорию с экспериментом и выкачать ее вручную. Пример для выкачивания директории с первым экспериментом приведен ниже:","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/\n!zip -r \"fast.zip\" \"fast\"\nfrom IPython.display import FileLink\nFileLink(r'fast.zip')","metadata":{"execution":{"iopub.status.busy":"2022-12-22T20:45:58.322675Z","iopub.execute_input":"2022-12-22T20:45:58.323377Z","iopub.status.idle":"2022-12-22T20:46:05.719235Z","shell.execute_reply.started":"2022-12-22T20:45:58.323334Z","shell.execute_reply":"2022-12-22T20:46:05.718089Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/kaggle/working\n  adding: fast/ (stored 0%)\n  adding: fast/keras_metadata.pb (deflated 95%)\n  adding: fast/saved_model.pb (deflated 92%)\n  adding: fast/variables/ (stored 0%)\n  adding: fast/variables/variables.index (deflated 79%)\n  adding: fast/variables/variables.data-00000-of-00001 (deflated 7%)\n  adding: fast/assets/ (stored 0%)\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fast.zip","text/html":"<a href='fast.zip' target='_blank'>fast.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"удалить лишние директории или файлы в output тоже легко:","metadata":{}},{"cell_type":"code","source":"!rm -r ../input/tennistrackingassignment/test/cache/1_1.npz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Для реализации загрузки данных рекомендуется использовать облачное хранилище google drive и пакет gdown для скачивания файлов. Пример подобного использования приведен ниже:\n1. загружаем файл в google drive (в данном случае, это npz архив, содержащий один numpy массив по ключу 'w')\n2. в интерфейсе google drive открываем доступ на чтение к файлу по ссылке и извлекаем из ссылки id файла\n3. формируем url для скачивания файла\n4. с помощью gdown скачиваем файл\n5. распаковываем npz архив и пользуемся numpy массивом\n\n<font color=\"red\">\nОбратите внимание, что для корректной работы нужно правильно определить id файла. В частности, в ссылке https://drive.google.com/file/d/1kZ8CC-zfkB_TlwtBjuPcEfsPV0Jz7IPA/view?usp=sharing id файла заключен между ...d/ b /view?... и равен 1kZ8CC-zfkB_TlwtBjuPcEfsPV0Jz7IPA\n</font>","metadata":{}},{"cell_type":"code","source":"import gdown\n\nid = '1kZ8CC-zfkB_TlwtBjuPcEfsPV0Jz7IPA'\nurl = f'https://drive.google.com/uc?id={id}'\noutput = 'sample-weights.npz'\ngdown.download(url, output, quiet=False)\n\nimport numpy as np\n\nweights = np.load('/kaggle/working/sample-weights.npz')['w']\nprint(weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef encoder_block(input, num_filters, add_max_pool_layer=True):\n    x = conv_block(input, num_filters)\n    if add_max_pool_layer:\n        p = MaxPool2D((2, 2))(x)\n    else:\n        p = x\n    return x, p\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_unet(input_shape, n_classes):\n    inputs = Input(input_shape)\n\n    s1, p1 = encoder_block(inputs, 64)\n    s2, p2 = encoder_block(p1, 128)\n    s3, p3 = encoder_block(p2, 256)\n    s4, p4 = encoder_block(p3, 512)\n\n    b1 = conv_block(p4, 1024)\n    \n    d1 = decoder_block(b1, s4, 512)\n    d2 = decoder_block(d1, s3, 256)\n    d3 = decoder_block(d2, s2, 128)\n    d4 = decoder_block(d3, s1, 64)\n    \n    if n_classes == 1:\n        activation = 'sigmoid'\n    else:\n        activation = 'softmax'\n\n    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)  \n\n    model = Model(inputs, outputs, name=\"U-Net\")\n    return model\n\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-2,\n    decay_steps=10000,\n    decay_rate=0.9)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n\n\nmy_unet = build_unet((384, 640, 9), n_classes=1)\nmy_unet.compile(optimizer=optimizer, loss=weighted_cross_entropy(10), metrics=[\"accuracy\"])\n# my_unet.summary()\n     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def TrackNet( n_classes=1,  input_height=360, input_width=640): # input_height = 360, input_width = 640\n\n    imgs_input = Input(shape=(input_height,input_width, 9))\n\n    #layer1\n    x = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same')(imgs_input)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer2\n    x = Conv2D(64, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer3\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n    #layer4\n    x = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer5\n    x = Conv2D(128, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer6\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n\n    #layer7\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same')(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer8\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer9\n    x = Conv2D(256, (3, 3), kernel_initializer='random_uniform', padding='same',   )(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer10\n    x = MaxPooling2D((2, 2), strides=(2, 2),   )(x)\n\n    #layer11\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer12\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer13\n    x = ( Conv2D(512, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer14\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer15\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer16\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer17\n    x = ( Conv2D( 256, (3, 3), kernel_initializer='random_uniform', padding='same',  ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer18\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer19\n    x = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer20\n    x = ( Conv2D( 128 , (3, 3), kernel_initializer='random_uniform', padding='same' ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer21\n    x = ( UpSampling2D( (2,2),  ))(x)\n\n    #layer22\n    x = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'  ,   ))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer23\n    x = ( Conv2D( 64 , (3, 3), kernel_initializer='random_uniform', padding='same'))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    #layer24\n    x = ( Conv2D( n_classes , (3, 3) , kernel_initializer='random_uniform', padding='same'))(x)\n    x = ( Activation('relu'))(x)\n    x = ( BatchNormalization())(x)\n\n    o_shape = Model(imgs_input, x).output_shape\n    print(\"layer24 output shape:\", o_shape[1],o_shape[2],o_shape[3])\n    #layer24 output shape: 256, 360, 640\n\n    OutputHeight = o_shape[1]\n    OutputWidth = o_shape[2]\n    \n    #reshape the size to (256, 360*640)\n    x = (Reshape((OutputHeight*OutputWidth,  -1)))(x)\n\n#     #change dimension order to (360*640, 256)\n#     x = (Permute((2, 1)))(x)\n    \n    #layer25\n    gaussian_output = (Activation('sigmoid'))(x)\n    \n    model = Model( imgs_input , gaussian_output )\n    model.outputWidth = OutputWidth\n    model.outputHeight = OutputHeight\n    \n    #show model's details\n    #model.summary()\n\n    return model\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}